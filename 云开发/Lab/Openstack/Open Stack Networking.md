

## 1 Introduction

IaaS platforms offer "virtual" version of different resources, including networks. As you know, Neutron is the Open Stack component in charge of virtualizing networks. Neutron defines only an API to create virtual networks. It is up to the administrator of the Open Stack cluster to choose an implementation for this API. Multiple solutions are possible, from purely software based to purely hardware based. Different vendor propose "Software Defined Networks" (SDN) implementation using specialized hardware. You will use a software only implementation based on Open vSwitch and VXLANs.

### 1.1 Provider and Self Service Networks

As stated in the often cited [NIST definition](https://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-145.pdf) of Cloud computing, "A consumer can **unilaterally provision** computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider." (Emphasis added.) In other words, the tenants (users) of a cloud computing platforms do not need to wait for a human intervention to create and exploit virtual resources.

The same document defines Infrastructure as a Service (IaaS) as: "The capability provided to the consumer is to provision processing, storage, **networks**, and other fundamental computing resources where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls)." (Emphasis added.)

Therefore one can argue that a "real" cloud computing platform must allow users to create and operate "virtual" networks, whose topology is independent of the underlying physical infrastructure. Open Stack does support these networks and the documentation calls them "self-service networks."

Open Stack also supports a less flexible solution, where its users cannot create networks directly but must instead rely on the operator of the physical infrastructure to provision the networks, typically in the form of VLANs. The documentation calls these networks "provider networks."

That being said, it is often the case that there is one "provider" network in an Open Stack installation. This physical network connects the platform to the Internet and is always administered by the operator of the physical infrastructure.

## 2 Physical Networks

Familiarize yourself with the physical network infrastructure by connecting to each machine (either from the controller or from your own machine) and listing all the available network interfaces with the command `ip a` (which is the short version of `ip address`).

Make sure to delete any existing VM in your infrastructure to remove all the interfaces used by them. In this case, on each machine you should have the four physical interfaces `enp1s0f0, enp1s0f1, enp1s0f2, enp1s0f3`, the loopback interface (`lo`) and the `eno1` interface, which is used to manage the lab computers of the lab and which you must not use. (The `eno1` interface should be `DOWN`and it should not have any IP address associated to if. If this is not the case, call the instructor.)

On each machine, the `enp1s0f0` interface is connected to the VLAN 2 on the TP-Link switch, i.e., the management VLAN. This network allows the administrator (that would be you) to connect to the physical machines without any interference from the traffic generated by the VMs.

The `enp1s0f1` interface is connected to the VLAN 3 on the TP-Link switch, i.e., the provider (or "external") network, which is connected to the outside world. This interface does not have an IP address because it handles the North-South traffic for the VMs, i.s., the traffic to/from the Internet and the VMs. The corresponding packets will use the IP addresses ("Floating IPs" more on that later) of the VMs.

The `enp1s0f2` is connected as well to the VLAN 3 and it does have an IP address. This is an "implementation detail." For technical reasons (only one Ethernet cable connected to the school network for each bench), we are forced to use this network to give access to the Internet to the **physical machines**. This is needed, above all, to download and install software. It can also be used to access the physical machines from anywhere on the school network. Usually the management network is connected to the Internet through a NAT (and possibly a proxy) in order to provide network connectivity to the physical machines.

The `enp1s0f3` interface is connected to the VLAN 4 that carries the traffic between the VMs (East-West), including the traffic to/from the virtual routers, as discussed below.

On the compute nodes, you also have the `virbr0` virtual bridge as well as the `virbr0-nic`. Both should be in the `DOWN` state. You can ignore them. Kvm, the hypervisor running the VMs, creates this bridge by default but it is not used by Open Stack.

You might also have an additional bridge on the controller and/or on one or more of the compute nodes, depending on the past usage of your platform.

Draw a diagram showing the connections between the physical machines. For each machine include the name of interface, its IP address (if present) and MAC address (just the first byte and the last two bytes). You can use a sheet of paper or any other tool (tablet, drawing program, etc.) that is fast and easy to use for you.

Each student must draw this figure and uploaded it to Moodle.

## 3 Open vSwitch

Open vSwitch provides the network implementation for the OpenStack platform that you are using for this class. Open vSwitch, as its names suggests, is an open source implementation of a virtual Ethernet switch. It actually provides more functionalities than a simple Ethernet switch but most of them are outside the scope of this class (including its OpenFlow features).

You might have already noticed that the output of the `ip a` command includes the following four interfaces: `ovs-system`, `br-int`, `br-provider`, and `br-tun`. The `ovs-system` does not correspond to an actual switch and you can consider it as an implementation detail (in other words, just ignore it). The other three correspond to three virtual switches that are present "inside" each node (including the controller).

You can use the command `ovs-vsctl show` (as root) to have more details about these switches. You can ignore the first four lines of the description of each switch (starting with: `Controller`, `is_connected`, `fail_mode`, and `datapath_type`). Each switch has several "Ports," each one containing an "Interface." It is possible for one port to have multiple interfaces but OpenStack does not use this feature. Each switch has an interface with the same name of the switch. This port is an implementation detail, just ignore it.

The three switches on each node are interconnected by interfaces of type "patch." You can determine the connections between interfaces thanks to the line `options: {peer=INTERFACE_NAME}`, where `INTERFACE_NAME` is the name of the other interface connected to the one containing the `options` line.

For each physical machine, Draw a diagram showing the three switches inside it and their connections. If you use pen and paper, use a different page for each machine and put the three switches in the middle of the page as you will add interfaces and connections throughout this lab. No need to write the name of the interfaces connecting each switch to another. 

Do not upload these figures to Moodle now. You will do it later on, after you have added more elements to them.

## 4 Exploring OpenStack Networks and Ports.

So far, you have looked at (par of) the "implementation" of the networking part of OpenStack. It is now time to start looking at what (virtual) resources exist in OpenStack.

On the controller, "source" the configuration file with `source admin-openrc` (or `source demo-openrc`) **in the home directory of the user `user`**. If you source the admin rc (configuration) file, all the OpenStack commands that you will type in the same shell will be executed with administrator privileges. While if you source the demo rc file, all the commands will have the same privileges/access as the "demo" user. **Note that the source command is valid only in the same terminal window (or tab) where it was issued.** In other words, if you open another terminal (or tab) and/or if you close the terminal/tab by mistake, you must issues the source command again.

After you have sourced the admin configuration file, you can ask OpenStack for the list of all existing networks, subnets, and interfaces with the following two commands:

```
> openstack network list
> openstack subnet list
> openstack port list
```

Note that, in this case, "port" refers to the virtual equivalent of a network card. It has nothing to do with transport layer ports (like TCP port 80 for a web server, or TCP port 22 for the ssh server).

Unless something went wrong with your bench, you should have one network, one subnet, and two ports. Your goal is now to determine where these two ports are actually "implemented." As the name of the ports implies (use `openstack port show [port-id]` to see the names if they are not shown with the `openstack port list` command), they are two "DHCP" servers that are running inside a dedicated namespace on two of your compute nodes. You might have noticed that the output of `ovs-vsctl` showed that two`br-int` switches have an internal interface whose name starts with `tap`. Find these interfaces and compare their name (the part after `tap`) with the beginning of the ID of the OpenStack ports. Do you notice anything?

Use the command `ip netns` on the two machines with these interfaces to get a list of network namespaces. There should be one namespace only, whose name starts with `qdhcp`. Next, use the command `ip netns exec NAMESPACE_NAME ip a` to run the `ip a` command inside that namespace (you must use sudo for this to work). Do you recognize one of the IP addresses? What about the name of the corresponding interface? And the MAC address: is it the same one as in the output of the `opesntack port list`command?

Download the sample csv file from `www.cloud.rennes.enst-bretagne.fr/files/openstack-network-lab/ports.csv`, add a line for each interface with the corresponding information (ID, IP address, MAC address, device, physical machine). You can find the name of the device in the output of the `ip netns exec` command. It starts with `tap` and it is the same name as the interface in the output of `ovs-vsctl`.

## 5 Provisioning Networks and VMs

For the time being, there are no VMs running on your platform. In order to study and analyze how Open Stack implements network communications to/from (and between) VMs, you first need to create a few VMs.

For security reasons, it is not possible to login using a username and password on the VMs created using the images available on your OpenStack installation. You must instead tell OpenStack which public key to inject into the `authorized_keys` file so that the corresponding user will be authorized to login using the matching private key. (By default, Ubuntu based images inject the keys in the `/home/ubuntu/.ssh/authorized_keys` file. Therefore you must login as the 'ubuntu' user.)

Unless you already have a public/private key pair that you would like to use, generate one with the following command (either on the controller or on your own computer) (feel free not to put a passphrase on the key, just hit enter when asked for the passphrase):

```
> ssh-keygen -f ~/.ssh/os-vms
```

Next, use the command below to upload the public key to OpenStack, so that it can inject it into the VMs. But first you must source the **demo** configuration file.

```
> openstack keypair create --public-key ~/.ssh/os-vms.pub vmkey
```

### 5.1 Heat Templates

As stated in the OpenStack documentation (https://docs.openstack.org/heat/stein/), /"Heat is a service to orchestrate composite cloud applications using a declarative template format through an OpenStack-native REST API"/. What does it mean? Instead of executing multiple OpenStack Shell commands to provision your resources (remember the first lab), you will describe all the OpenStack resources you need in a YAML file (called a heat template) and you will use a `stack create` command to provision your infrastructure.

For the sake of simplicity and expediency, the teaching staff has already written all the stack files. Download `net-1.yml`, `net-2.yml`, and `vm_template.yaml` from `http://www.cloud.rennes.enst-bretagne.fr/files/openstack-network-lab`. Note that the extension of the last file is `.yaml` and not `.yml` as the others. This is a limitation of Heat.

Open `net-1.yml` in your favorite editor (or just use `less`) to examine it. Note how it creates different `resources`. In particular `private_net_1` (of type `OS::Neutron::Net`) and `private_subnet_1` (of type `S::Neutron::Subnet`): the first is a **layer 2** network, while the second is an **IP subnet** (one could say a "layer 3 network). Note how the subnet is associated to the layer 2 network, thanks to the `network_id: { get_resource: private_net_1 }` property of the subnet resource. The IP prefix of the subnet is `192.168.153.0/24`.

The template creates a virtual router (`gateway_router_1`) with two interfaces: one on the external network (`external_gateway_info` property of the `gateway_router_1` resource), and a second one on the `private_subnet_1`(`gateway_router_interface_1` resource).

The template creates three VMs, as well as the corresponding "network cards" (resource type `OS::Neutron::Port`). The first two VMs (VM1 and VM2) are connected to the private network and, as such, they cannot be contacted **from** outside the private network. They can connect **to** the outside because the virtual router is also a NAT. In order to reach them from the outside, one needs to associate "floating IPs" to them. This is why the template creates two such resources. The floating IPs belong to the external network, which is under the control of the administrator, that is why the template cannot specify the value of the floating IP addresses.

The third VM (VM3) is directly connected to the external network, therefore it does not need a floating IP. But it will not be able to connect to the two other VMs directly. It will have to "go through" the virtual router.

The template creates also a "server group," this is just an implementation detail, to ask OpenStack to schedule the three VMs on different physical machines, as long as there is enough space.

Examine the content of the `net-2.yml` file as well, noting the similarities and differences with the first template.

Use the following commands to instantiate the first template (remember to source the OpenStack configuration file for the user demo, if you have not yet done it in the shell that you are using, and make sure to run the command in the folder containing the yaml files that you just downloaded:

```
> openstack stack create -t net-1.yml net-1
```

If, for some reason, you have changed the name of the keypair above, you can add `--parameter ssh_key=name_of_our_key`**before** `net-1`. This is the names of the stack and must be at the end of the command.

## 6 Network Implementation Overview

As previously mentioned, there are many different ways of implementing networking services in OpenStack. In this section, we give a brief overview of the implementation that you are using.

### 6.1 Tenant Abstractions

The overwhelming majority of cloud computing platforms (including the large public ones) offer layer 2 and 3 networks, with Ethernet at layer 2 and IP at layer 3. Such a solution has several advantages: it is fairly simple and familiar to most technical users and, above all, being equivalent to the typical enterprise network, it allows to easily migrate applications from physical to virtual infrastructure with little or no modifications. From the point of view of the OS and of the applications, there are no differences between running on a physical machine rather than a VM. The Ethernet and IP protocol behave exactly the same way as on a physical infrastructure. The only potential problem is that, depending on the performance of the virtualization solution, the performance can be lower than a corresponding physical network. The latency can also be higher, depending again, on the specific implementation and its load.

Recall that that heat template created a "network." In OpenStack a "network" is a layer 2 Ethernet network. You can think about it as a "broadcast domain." As such it must also correspond to an IP subnet. This is why the heat template created an IP subnet as well, with the associated prefix (192.168.153.0/24).

### 6.2 VXLANs

Your OpenStack platform uses VXLANs to implement the private networks. [RFC 7348](https://tools.ietf.org/html/rfc7348) describes the VXLAN standard. The basic idea is to encapsulate Ethernet frames (those generated by the VMs) inside UDP packets to forward them from one physical machine to another. Each VXLAN is identified by a unique identifier called VXLAN Network Identifier (VNI) (or VXLAN Segment ID). The VXLAN header, inside the UDP packet, contains this value, allowing the receiving node to determine to which VXLAN (or segment) the internal Ethernet frame corresponds to.

Each physical machine hosting a VM connected to a private network (e.g., `private_net_1`) contains a VXLAN Tunnel End Point (VTEP). This is in charge of taking the Ethernet frames generated by the VMs and encapsulating them in an VXLAN header and then sending them inside a UDP packet to the physical node containing the destination VM (or virtual router). When receiving a VXLAN packet, the VTEP removes the VXLAN header and handles the inner Ethernet frame to the VM.

While the standard calls for using an IP multicast address associated with each VXLAN to forward Broadcast, Unknown, and Multicast (BUM) Ethernet frames, the implementation that you are using uses IP unicast by configuring each VTEP with the IP addresses of the physical machines.

Connect to the controller and to the three compute nodes. The output of the `ip a` command should now contain an interface whose names start with `vxlan`. Similarly, the output of `ovs-vsctl show` should now contain three new interfaces in the `br-tun` switch, as well as at least one extra interface (sometimes two) in the `br-int` switch.

Do you recognize a pattern in the `local_ip` and `remote_ip` options of the vxlan interfaces of the `br-tun` switch? To which physical network to the belong?

If you want to know more about VXLANs, you can read [part 1](http://www.definethecloud.net/vxlan-deep-dive/) and [part 2](http://www.definethecloud.net/vxlan-deep-divepart-2/) of a useful overview.

Execute again the OpenStack commands to obtain the list of networks, subnets, and ports. What do you observe? Add a line for each port to the `ports.csv` file and fill out the corresponding information.

Add the new switches and ports to your diagrams of each physical machine.

You can use the command `openstack server list` to obtain a list of VMs and their IP addresses. You should find two extra DHCP namespaces, and one new namespace (for the virtual router) on the controller. As well as one Linux bridge for each VM, whose name starts with `qbr` (shown in the output of `ip a`).

Upload the csv file and a your four diagrams to Moodle.

### 6.3 Layer 3 (L3) agent

The "Layer 3" agent (L3 agent) implements the virtual routers. It runs on the controller inside a network namespace. A namespace is a feature of the Linux kernel to limit which resources processes can see. Network namespaces, unsurprisingly, deal with network interfaces and also with routing tables.

Each virtual router corresponds to a different namespace.

Before proceeding, you can instantiate the second heat template, with the following command:

```
> openstack stack create -t net-2.yml net-2
```

Use the commands below to list the network namespaces on the controller, show the network interfaces, and the routing table inside each namespace (replace `<ns_name>` with each one of the network namespaces in the output of the first command.

Does the IP prefix associated with one of the interfaces inside the namespace match one of the IP prefixes in use on the private networks?

```
> ip netns ls
> sudo ip netns exec <ns_name> ip a
> sudo ip netns exec <ns_name> ip route
```

The virtual router uses the Linux iptables mechanism to implement the floating IPs using a "static NAT rule," which statically associates a floating IP address with the corresponding private IP address. Whenever the router receives an incoming packet addressed to one of the floating IPs, iptables replaces the destination address with the corresponding private IP address and forwards the packet on the vxlan.

You can use the command below to show the rules of the "`nat`" table inside the router namesapce. If the router does implement at least one floating IP, you should see a rule entitled "Chain neutron-l3-agent-float-snat" with the matching floating and private IP addresses.

```
ip netns exec <ns_name> iptables -t nat -L
```

### 6.4 ML2 plugin

You might recall having configured something about an "ML2 plugin" during the first OpenStack lab. The ML2 plugin is a software abstraction layer offering a unified interface independent of the underlying implementation.

For example, it supports multiple "types" of layer 2 networks: VXLAN, GRE, Geneve. It also supports different implementation "mechanisms": Linux Bridge, Open Virtual Switch, SRIOV. The same type of network (e.g., VXLAN) can be implemented by different mechanisms (e.g., Linux Bridge, Open vSwitch).

You can think of the ML2 plugin as the "system/network administrator" who configures the switches (and routers) on each node whenever you add/remove networks, subnets, and VMs.

## 7 Tracing Packet Flows

The goal of this section is to use `tcpdump` and Wireshark to trace the path taken by different types of network traffic.

For each one of the scenarios described below, start `tcpdump` on the interfaces traversed by the traffic generated. Copy the `pcap` files to the controller and analyze them with Wireshark. Once you can prove that you have been able to trace each step of traffic flow, call the instructor and show your results. 

Install Wireshark on the controller with the following commands:

```
> sudo apt update
> sudo apt install wireshark
```

It is possible to start tcpdump on a remote machine with ssh and forward its output to wireshark running on your controller. For example, the following command captures the packets on the `enp1s0f3` interface of compute1 (note that you have to type the password in the terminal for this to work):

```
> ssh user@compute1 sudo tcpdump -U -s0 -i enp1s0f3 -w - | wireshark -k -i -  
```

You can use wireshark to save the capture results on the controller. , You can ask tcpdump to ignore certain packets, for example, the following command asks tcpdump to ignore all the ssh packets:

```
> ssh user@compute1 sudo tcpdump -U -s0 -i enp1s0f3 -w - 'not port 22' | wireshark -k -i - 
```

It is also possible to capture packets on an interface inside a namespase (replace `<ns_name>` with the name of namespace and `<int_name>` with the name of the interface **inside the namespace**:

```
> ssh user@compute1 sudo ip netns exec <ns_name> tcpdump -U -s0 -i <int_name> -w -| wireshark -k -i -
```

### 7.1 East-West Traffic

First of all, using the administrator's credentials to make sure that `VM1` and `VM2` are running on different physical machines. If this is not the case, call the instructor. You can either use a browser to connect to `controller/hoizon` and then put "default" in the "Domain" field, "admin" in the "User Name" field and "usr" in the "Password" field, then click on "Admin" on the menu on the left of the page, a new sub-menu should appear so that you can click on "Compute" and then "Instances." You should then see a table with all the running VMs including on which physical machine ("Host") they are running. Another possibility is to source the admin configuration file in a terminal and execute the command `openstack server list --long --all-projects -f yaml` (the output will be in yaml, if you don't give this option the output is an extremely wide table that does not usually fit on the screen so that it's really hard to read).

If you prefer, you can use the commands below to start capturing packets with `tcpdump`. Specify the capture interface with the `-i`option. Always use the `-w` option to specify an output file so that you can further analyze the files with Wireshark.m You can capture packets on the physical machines and/or inside the VMs.

```
> sudo tcpdump -i <device> -w <file_name>
```

In case you want to run `tcpdump` "live", you can use the following commands to filter the packets that are shown:

```
> sudo tcpdump -i <device> -n icmp 
> sudo tcpdump -i <device> host <IP_address> 
```

#### 7.1.1 From VM1 to VM2

Connect to `VM1` using ssh and ping `VM2`. Capture packets on the following interfaces:

1. The tap interface corresponding to `VM2` (this is the traffic sent/received on the VM itself).
2. The physical interface of the compute node hosting `VM2` traversed by the ping messages.
3. The physical interface of the compute node hosting `VM1` traversed by the ping messages.
4. The tap interface corresponding to `VM1`

Create a folder called `east-west` containing the four capture files. In the same folder, put a text file with the list of all the interfaces (including the Open vSwitch ones) traversed by the ping messages from `VM1` to `VM2`.

Are the packets captured in steps 2 and 3 above the same? 

### 7.2 North-South Traffic

#### 7.2.1 From an Outside Machine to VM1

If you have a laptop connected to the school WiFi network, use it to ping `VM1`. Otherwise, ask another group to let them use one of their computers for this step. Obviously, you have to ping the floating IP associated to `VM1` (you can get it with `openstack server list`).

Based on your diagrams and what you know about OpenStack networking (in particular how floating IPs are implemented) determine all the physical interfaces that will be traversed by the ping messages. Capture traffic on each one of them. 

Create a folder called `north-south` containing the capture files. In the same folder, put a text file with the list of all the interfaces (including the Open vSwitch ones) traversed by the ping messages from the moment the ping messages reach your bench until they reach `VM1`.

Create an archive containing the `east-west` and `north-south` folders and uploaded it Moodle.

### 7.3 More Tests

#### 7.3.1 From VM1 to VM3

Connect to `VM2` using ssh and ping `VM3`. As before, capture your packets along the way. This case is not as easy as you might think! [Hint: is `VM3` connected to the same network as `VM1`?]

#### 7.3.2 Multiple Tenants

There are three pre-configured users in your OpenStack platform: `admin`, `demo`, and `show`. You can find the corresponding rc files in the home directory of the user `user` on the controller. Source the rc file of the `show` user and create a keypair like you did for the `demo`user (you can use the same public/private key pair). Download the `net-3.yml` from `http://www.cloud.rennes.enst-bretagne.fr/files/openstack-network-lab`, and inspect it. Note how it uses some of the same IP addresses as `net-1.yml`, this is why it is essential that you issue the `stack create` command only after you have sourced the rc file of the `show` user.

Observe how Open vSwitch has configured the `br-tun` bridge with the appropriate VLANs so that traffic of the two different users is isolated.

Ping `VM2` from `VM1` (of the `show` user). As before, capture your packets along the way.

Author: Alberto Blanc, Christophe Couturier, Jean-Pierre Le Narzul

Created: 2022-11-20 Sun 21:54

[Validate](https://validator.w3.org/check?uri=referer)